{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install dialogflow gensim==3.6.0 annoy tqdm stop_words pymorphy2 python-telegram-bot==13.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install pandarallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic_16_nlp_1.jpg' width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic_16_nlp_2.jpg' width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic_16_nlp_3.jpg' width=1000>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==1.4.0\r\n",
      "aiohttp==3.8.3\r\n",
      "aiosignal==1.3.1\r\n",
      "annoy==1.17.1\r\n",
      "anyio @ file:///opt/concourse/worker/volumes/live/485b0f52-1188-482a-6285-65a36c8fa8a6/volume/anyio_1644481714856/work/dist\r\n",
      "appnope @ file:///opt/concourse/worker/volumes/live/6ca6f098-d773-4461-5c91-a24a17435bda/volume/appnope_1606859448531/work\r\n",
      "APScheduler==3.6.3\r\n",
      "argon2-cffi @ file:///opt/conda/conda-bld/argon2-cffi_1645000214183/work\r\n",
      "argon2-cffi-bindings @ file:///opt/concourse/worker/volumes/live/42cf1b28-e71f-45ed-47b2-50f828088636/volume/argon2-cffi-bindings_1644569709119/work\r\n",
      "asttokens @ file:///opt/conda/conda-bld/asttokens_1646925590279/work\r\n",
      "astunparse==1.6.3\r\n",
      "async-timeout==4.0.2\r\n",
      "attrs @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_33k1uces4n/croot/attrs_1668696162258/work\r\n",
      "backcall @ file:///home/ktietz/src/ci/backcall_1611930011877/work\r\n",
      "beautifulsoup4 @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_croot-cdiouih5/beautifulsoup4_1650462164803/work\r\n",
      "bleach @ file:///opt/conda/conda-bld/bleach_1641577558959/work\r\n",
      "cachetools==4.2.2\r\n",
      "catboost==1.1.1\r\n",
      "certifi @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_477u68wvzm/croot/certifi_1671487773341/work/certifi\r\n",
      "cffi @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_1b0qzba5nr/croot/cffi_1670423213150/work\r\n",
      "charset-normalizer==2.1.1\r\n",
      "click==8.1.3\r\n",
      "cloudpickle==2.2.1\r\n",
      "comm @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_0b9r9i3b7k/croot/comm_1671231125581/work\r\n",
      "contourpy==1.0.7\r\n",
      "cryptography==39.0.0\r\n",
      "cycler==0.11.0\r\n",
      "datasets==2.12.0\r\n",
      "DAWG-Python==0.7.2\r\n",
      "debugpy @ file:///opt/concourse/worker/volumes/live/fb0af3c0-b0a4-42c5-46e7-6ce9c84da602/volume/debugpy_1637091827531/work\r\n",
      "decorator @ file:///opt/conda/conda-bld/decorator_1643638310831/work\r\n",
      "defusedxml @ file:///tmp/build/80754af9/defusedxml_1615228127516/work\r\n",
      "dialogflow==2.0.0\r\n",
      "dill==0.3.6\r\n",
      "docopt==0.6.2\r\n",
      "eli5==0.13.0\r\n",
      "entrypoints @ file:///opt/concourse/worker/volumes/live/78b2c4ae-da17-4aa7-7fdc-138f840abf07/volume/entrypoints_1649926486598/work\r\n",
      "et-xmlfile==1.1.0\r\n",
      "evaluate==0.4.0\r\n",
      "executing @ file:///opt/conda/conda-bld/executing_1646925071911/work\r\n",
      "fastjsonschema @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b5c1gee32t/croots/recipe/python-fastjsonschema_1661368622875/work\r\n",
      "filelock==3.9.0\r\n",
      "Flask==2.2.2\r\n",
      "flatbuffers==23.5.26\r\n",
      "flit_core @ file:///opt/conda/conda-bld/flit-core_1644941570762/work/source/flit_core\r\n",
      "fonttools==4.38.0\r\n",
      "frozenlist==1.3.3\r\n",
      "fsspec==2022.11.0\r\n",
      "fst-pso==1.8.1\r\n",
      "FuzzyTM==2.0.5\r\n",
      "gast==0.4.0\r\n",
      "gensim==3.6.0\r\n",
      "google-api-core==1.34.0\r\n",
      "google-auth==2.19.0\r\n",
      "google-auth-oauthlib==1.0.0\r\n",
      "google-cloud-dialogflow==2.21.0\r\n",
      "google-pasta==0.2.0\r\n",
      "googleapis-common-protos==1.59.0\r\n",
      "graphviz==0.20.1\r\n",
      "grpcio==1.54.2\r\n",
      "grpcio-status==1.48.2\r\n",
      "h11==0.14.0\r\n",
      "h5py==3.9.0\r\n",
      "httpcore==0.17.2\r\n",
      "httpx==0.24.1\r\n",
      "huggingface-hub==0.14.1\r\n",
      "idna @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_00jf0h4zbt/croot/idna_1666125573348/work\r\n",
      "imageio==2.25.1\r\n",
      "importlib-metadata==6.0.0\r\n",
      "intervaltree==3.1.0\r\n",
      "ipykernel @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_4dybncc18w/croot/ipykernel_1671488388285/work\r\n",
      "ipymarkup==0.9.0\r\n",
      "ipython @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_444myk5dgj/croot/ipython_1670919323426/work\r\n",
      "ipython-genutils @ file:///tmp/build/80754af9/ipython_genutils_1606773439826/work\r\n",
      "ipywidgets==8.0.4\r\n",
      "itsdangerous==2.1.2\r\n",
      "jedi @ file:///opt/concourse/worker/volumes/live/89be3eb0-a85c-453b-67dd-f706f2fa4c43/volume/jedi_1644315269953/work\r\n",
      "Jinja2 @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_6adj7x0ejx/croot/jinja2_1666908137966/work\r\n",
      "joblib==1.2.0\r\n",
      "jsonschema @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_d832da7jx3/croots/recipe/jsonschema_1663375475386/work\r\n",
      "jupyter-server @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_031akrjssy/croot/jupyter_server_1671707631142/work\r\n",
      "jupyter_client @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_8dsejoebqs/croot/jupyter_client_1671703053245/work\r\n",
      "jupyter_core @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_c1_m4s19su/croot/jupyter_core_1672332229812/work\r\n",
      "jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work\r\n",
      "jupyterlab-widgets==3.0.5\r\n",
      "keras==2.13.1\r\n",
      "kiwisolver==1.4.4\r\n",
      "libclang==16.0.6\r\n",
      "lightgbm==3.3.5\r\n",
      "lime==0.2.0.1\r\n",
      "llvmlite==0.39.1\r\n",
      "lxml @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_1902c961-4bd2-4871-a3c5-70b7317a6521kpj7nz2o/croots/recipe/lxml_1657545138937/work\r\n",
      "Markdown==3.4.4\r\n",
      "MarkupSafe @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_d4a9444f-bd4c-4043-b47d-cede33979b0fve7bm42r/croots/recipe/markupsafe_1654597878200/work\r\n",
      "matplotlib==3.6.3\r\n",
      "matplotlib-inline @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_9ddl71oqte/croots/recipe/matplotlib-inline_1662014471815/work\r\n",
      "miniful==0.0.6\r\n",
      "mistune @ file:///opt/concourse/worker/volumes/live/4217afd5-dad1-438d-6f79-e4992ccda0e5/volume/mistune_1607364880245/work\r\n",
      "multidict==6.0.4\r\n",
      "multiprocess==0.70.14\r\n",
      "navec==0.10.0\r\n",
      "nbclassic @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_15tvfkx5gf/croot/nbclassic_1668174968985/work\r\n",
      "nbclient @ file:///opt/concourse/worker/volumes/live/fcea0efc-2a08-48fd-5c55-85ef78e0ea28/volume/nbclient_1650308406463/work\r\n",
      "nbconvert @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_8fyzuglni_/croot/nbconvert_1668450649428/work\r\n",
      "nbformat @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_2daun1fill/croot/nbformat_1670352339504/work\r\n",
      "nest-asyncio @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_64pfm74mxq/croot/nest-asyncio_1672387129786/work\r\n",
      "networkx==3.0\r\n",
      "nltk==3.8.1\r\n",
      "notebook @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_0cdyriuhi_/croot/notebook_1668179888986/work\r\n",
      "notebook_shim @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_e9s6zsmlb7/croot/notebook-shim_1668160584892/work\r\n",
      "numba==0.56.4\r\n",
      "numpy==1.23.5\r",
      "\r\n",
      "oauthlib==3.2.2\r\n",
      "openai==0.27.6\r\n",
      "opencv-python==4.8.0.74\r\n",
      "openpyxl==3.1.2\r\n",
      "opt-einsum==3.3.0\r\n",
      "packaging @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_bet5qdixgt/croot/packaging_1671697440883/work\r\n",
      "pandarallel==1.6.5\r\n",
      "pandas==1.5.2\r\n",
      "pandocfilters @ file:///opt/conda/conda-bld/pandocfilters_1643405455980/work\r\n",
      "parso @ file:///opt/conda/conda-bld/parso_1641458642106/work\r\n",
      "patsy==0.5.3\r\n",
      "pexpect @ file:///tmp/build/80754af9/pexpect_1605563209008/work\r\n",
      "pickleshare @ file:///tmp/build/80754af9/pickleshare_1606932040724/work\r\n",
      "Pillow==9.4.0\r\n",
      "platformdirs @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_7fs8_2xgrm/croots/recipe/platformdirs_1662711383474/work\r\n",
      "plotly==5.12.0\r\n",
      "prometheus-client @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_19kjbndib7/croots/recipe/prometheus_client_1659455105394/work\r\n",
      "prompt-toolkit @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_82emz7mook/croot/prompt-toolkit_1672387300396/work\r\n",
      "proto-plus==1.22.2\r\n",
      "protobuf==3.20.3\r\n",
      "psutil @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_c9b604bf-685f-47f6-8304-238e4e70557e1o7mmsot/croots/recipe/psutil_1656431274701/work\r\n",
      "ptyprocess @ file:///tmp/build/80754af9/ptyprocess_1609355006118/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\r\n",
      "pure-eval @ file:///opt/conda/conda-bld/pure_eval_1646925070566/work\r\n",
      "py4j==0.10.9.7\r\n",
      "pyarrow==10.0.1\r\n",
      "pyasn1==0.5.0\r\n",
      "pyasn1-modules==0.3.0\r\n",
      "pyconll==3.1.0\r\n",
      "pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work\r\n",
      "pyFUME==0.2.25\r\n",
      "Pygments @ file:///opt/conda/conda-bld/pygments_1644249106324/work\r\n",
      "pymorphy2==0.9.1\r\n",
      "pymorphy2-dicts-ru==2.4.417127.4579844\r\n",
      "pyparsing==3.0.9\r\n",
      "pyrsistent @ file:///opt/concourse/worker/volumes/live/76cffa60-bd33-4155-4e83-ea03c38b1294/volume/pyrsistent_1636111020441/work\r\n",
      "pyspark==3.4.0\r\n",
      "python-dateutil @ file:///tmp/build/80754af9/python-dateutil_1626374649649/work\r\n",
      "python-telegram-bot==13.3\r\n",
      "pytz==2022.7.1\r\n",
      "pytz-deprecation-shim==0.1.0.post0\r\n",
      "PyWavelets==1.4.1\r\n",
      "PyYAML==6.0\r",
      "\r\n",
      "pyzmq @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_15f7a459-ad98-422b-b8da-cbf1f626e2115nt0ocwy/croots/recipe/pyzmq_1657724193704/work\r\n",
      "razdel==0.5.0\r\n",
      "regex==2022.10.31\r\n",
      "requests==2.28.2\r\n",
      "requests-oauthlib==1.3.1\r\n",
      "responses==0.18.0\r\n",
      "rsa==4.9\r\n",
      "scikit-image==0.19.3\r\n",
      "scikit-learn==1.2.0\r\n",
      "scikit-uplift==0.5.1\r\n",
      "scipy==1.10.0\r\n",
      "seaborn==0.12.2\r\n",
      "Send2Trash @ file:///tmp/build/80754af9/send2trash_1632406701022/work\r\n",
      "sentencepiece==0.1.97\r\n",
      "shap==0.41.0\r\n",
      "simpful==2.9.0\r\n",
      "six @ file:///tmp/build/80754af9/six_1644875935023/work\r\n",
      "slicer==0.0.7\r\n",
      "slovnet==0.6.0\r\n",
      "smart-open==6.3.0\r\n",
      "sniffio @ file:///opt/concourse/worker/volumes/live/38ca9e9e-09d1-4d43-5a0f-b546422e7807/volume/sniffio_1614030472707/work\r\n",
      "sortedcontainers==2.4.0\r\n",
      "soupsieve @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_14fb2zs6e3/croot/soupsieve_1666296397588/work\r\n",
      "SpeechRecognition==3.9.0\r\n",
      "srt==3.5.3\r\n",
      "stack-data @ file:///opt/conda/conda-bld/stack_data_1646927590127/work\r\n",
      "statsmodels==0.14.0\r\n",
      "stop-words==2018.7.23\r\n",
      "tabulate==0.9.0\r\n",
      "tenacity==8.1.0\r\n",
      "tensorboard==2.13.0\r\n",
      "tensorboard-data-server==0.7.1\r\n",
      "tensorflow==2.13.0\r\n",
      "tensorflow-estimator==2.13.0\r\n",
      "tensorflow-io-gcs-filesystem==0.32.0\r\n",
      "termcolor==2.3.0\r\n",
      "terminado @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_18_p3gbeio/croot/terminado_1671751835656/work\r\n",
      "tf-explain==0.3.1\r\n",
      "threadpoolctl==3.1.0\r\n",
      "tifffile==2023.2.3\r\n",
      "tinycss2 @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_56dshjmms6/croot/tinycss2_1668168824483/work\r\n",
      "tokenizers==0.13.2\r\n",
      "torch==1.12.1\r\n",
      "torchaudio==0.12.1\r\n",
      "torchsummary==1.5.1\r\n",
      "torchvision==0.13.1\r\n",
      "tornado @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_1fimz6o0gc/croots/recipe/tornado_1662061695695/work\r\n",
      "tqdm==4.65.0\r\n",
      "traitlets @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_0dtilxc0bw/croot/traitlets_1671143889152/work\r\n",
      "transformers==4.29.2\r\n",
      "typing_extensions @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_4b7xacf029/croot/typing_extensions_1669923792404/work\r\n",
      "tzdata==2022.7\r\n",
      "tzlocal==4.2\r\n",
      "urllib3==1.26.14\r\n",
      "vosk==0.3.44\r\n",
      "wcwidth @ file:///Users/ktietz/demo/mc3/conda-bld/wcwidth_1629357192024/work\r\n",
      "webencodings==0.5.1\r\n",
      "websocket-client @ file:///opt/concourse/worker/volumes/live/5baed9cd-40fb-4fbe-6721-9568cdd0f2d7/volume/websocket-client_1614804245073/work\r\n",
      "websockets==11.0.2\r\n",
      "Werkzeug==2.2.2\r\n",
      "widgetsnbextension==4.0.5\r\n",
      "wikipedia==1.4.0\r\n",
      "wrapt==1.15.0\r\n",
      "xgboost==1.7.3\r\n",
      "xxhash==3.2.0\r\n",
      "yarl==1.8.2\r\n",
      "zipp==3.11.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.   RESTART KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xAhRcgdAVJzn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from telegram.ext  import Updater, CommandHandler, MessageHandler, Filters\n",
    "import string\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "import annoy\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "    \n",
    "tqdm.pandas()\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmap\n",
    "import re\n",
    "\n",
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скачать файл Otvety.txt по ссылке  (1,7G)\n",
    "# https://drive.google.com/file/d/1DQL9ybca4USImUDaxxHmkIZNmClKBtKG/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JBnSQav-VJz6"
   },
   "outputs": [],
   "source": [
    "# Преобразование файла аопросов-ответов в строчный вид\n",
    "if not os.path.isfile('prepared_answers.txt'):\n",
    "\n",
    "    question = None\n",
    "    written = False\n",
    "\n",
    "    with open(\"prepared_answers.txt\", \"w\") as fout:  \n",
    "        with open(\"Otvety.txt\", \"r\") as fin:\n",
    "            for line in tqdm_notebook(fin):\n",
    "                if line.startswith(\"---\"):\n",
    "                    written = False\n",
    "                    continue\n",
    "                if not written and question is not None:\n",
    "                    fout.write(question.replace(\"\\t\", \" \").strip() + \"\\t\" + line.replace(\"\\t\", \" \"))\n",
    "                    written = True\n",
    "                    question = None\n",
    "                    continue\n",
    "                if not written:\n",
    "                    question = line.strip()\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qNovia_FVJz7"
   },
   "outputs": [],
   "source": [
    "# Препроцессинг текста\n",
    "def preprocess_txt(line):\n",
    "    spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "    return spls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для очистки текста из статьи https://habr.com/ru/articles/738176/ адаптированная под русский язык\n",
    "def clean_text(input_text):\n",
    "\n",
    "    # HTML-теги: первый шаг - удалить из входного текста все HTML-теги\n",
    "    clean_text = re.sub('<[^<]+?>', '', input_text)\n",
    "\n",
    "    # URL и ссылки: далее - удаляем из текста все URL и ссылки\n",
    "    clean_text = re.sub(r'http\\S+', '', clean_text)\n",
    "\n",
    "    #Эмоджи и эмотиконы: используем собственную функцию для преобразования эмоджи в текст\n",
    "    #Важно понимать эмоциональную окраску обрабатываемого текста\n",
    "    clean_text = emojis_words(clean_text)\n",
    "\n",
    "    # Приводим все входные данные к нижнему регистру\n",
    "    clean_text = clean_text.lower()\n",
    "\n",
    "    # Убираем все пробелы\n",
    "    # Так как все данные теперь представлены словами - удалим пробелы\n",
    "    clean_text = re.sub('\\s+', ' ', clean_text)\n",
    "\n",
    "    #Убираем специальные символы: избавляемся от всего, что не является \"словами\"\n",
    "    clean_text = re.sub('[^а-яА-ЯёЁ0-9\\s]', '', clean_text) #\n",
    "\n",
    "    # Записываем числа прописью: 100 превращается в \"сто\" (для компьютера)# не работает\n",
    "    temp = inflect.engine()\n",
    "    words = []\n",
    "    for word in clean_text.split():\n",
    "        if word.isdigit():\n",
    "            words.append(num2words(int(word), lang='ru'))\n",
    "        else:\n",
    "            words.append(word)\n",
    "    clean_text = ' '.join(words)\n",
    "\n",
    "        # Стоп-слова: удаление стоп-слов - это стандартная практика очистки текстов\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "    tokens = word_tokenize(clean_text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    clean_text = ' '.join(tokens)\n",
    "\n",
    "    # Знаки препинания: далее - удаляем из текста все знаки препинания\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', clean_text)\n",
    "\n",
    "    # И наконец - возвращаем очищенный текст\n",
    "    return clean_text\n",
    "\n",
    "# Функция для преобразования эмоджи в слова\n",
    "def emojis_words(text):\n",
    "\n",
    "    # Модуль emoji: преобразование эмоджи в их словесные описания\n",
    "    clean_text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "    # Редактирование текста путём замены \":\" и\" _\", а так же - путём добавления пробела между отдельными словами\n",
    "    clean_text = clean_text.replace(\":\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "def preproc_text(text):\n",
    "    res = str(text).strip()\n",
    "    res = re.sub(r\"\\s+\", \" \", res)\n",
    "    return res\n",
    "\n",
    "def build_data(data_q, data_ans):\n",
    "    data = []\n",
    "    for idx, texts in enumerate(data_q):\n",
    "        question = preproc_text(texts)\n",
    "        answer = preproc_text(data_ans.iloc[idx])\n",
    "        res = '\\nx:' + question + '\\ny:' + answer\n",
    "        data.append(res)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f32db27df42f4863a58d2d9996f088e3"
     ]
    },
    "id": "uvwpdubYVJz8",
    "outputId": "83dd0de8-4106-4dbc-c584-61c1a23cb9bd"
   },
   "outputs": [],
   "source": [
    "# Обработка текста\n",
    "\n",
    "sentences = []\n",
    "morpher = MorphAnalyzer()\n",
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(string.punctuation)\n",
    "c = 0\n",
    "\n",
    "file_path_from = 'prepared_answers.txt'\n",
    "file_path_to = 'Otvety2.txt'\n",
    "\n",
    "if not os.path.isfile(file_path_to):\n",
    "    \n",
    "    N = get_num_lines(file_path_from)\n",
    "    with open(file_path_to, mode = 'w') as fileto:\n",
    "        with open(file_path_from) as filefrom:\n",
    "            for k in tqdm(range(N)):\n",
    "                line = filefrom.readline()\n",
    "                if line == '': break\n",
    "                spls = preprocess_txt(line)\n",
    "                sentences.append(spls)\n",
    "                c += 1\n",
    "                if c > 500000000:\n",
    "                    break\n",
    "                fileto.write(' '.join(spls)+'\\n')\n",
    "    filefrom.close()\n",
    "    fileto.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1601d2fd14524a0a8e0c1a35103f4640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7016256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Загрузить результат\n",
    "\n",
    "sentences = []\n",
    "\n",
    "file_path_from = 'Otvety2.txt'\n",
    "if os.path.isfile(file_path_from):  \n",
    "    N = get_num_lines(file_path_from) \n",
    "    with open(file_path_to, mode = 'r') as filefrom:\n",
    "        for k in tqdm(range(N)):\n",
    "            line = filefrom.readline()\n",
    "            if line == '': break\n",
    "            sentences.append(line.split())\n",
    "    filefrom.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brconvert',\n",
       " 'brps',\n",
       " 'brв',\n",
       " 'brвампир',\n",
       " 'brвитание',\n",
       " 'brвсе',\n",
       " 'brгде',\n",
       " 'brдикобраз',\n",
       " 'brести',\n",
       " 'brзабористый']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = []\n",
    "_ = [vec.extend(x)  for x in sentences[:100]]\n",
    "vec = list(set(vec))\n",
    "vec.sort()\n",
    "vec[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'] = value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " ['вопрос', 'тдв', 'отдыхать', 'лично', 'советовать', 'завести'],\n",
       " ['хомячок'],\n",
       " ['мужик', 'йопарить', 'собачка', '50', 'кошка'],\n",
       " ['общение'],\n",
       " ['паучок'],\n",
       " ['пол', 'памытьbr', 'таг', 'тип', 'каво'],\n",
       " ['вообще', 'пообщаться'],\n",
       " ['советовать', 'сися', 'завести']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = sentences[:50_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zwXvXkJBVJ0B"
   },
   "outputs": [],
   "source": [
    "# Обучим модель FastText\n",
    "\n",
    "file_path_from = 'ft_model'\n",
    "if not os.path.isfile(file_path_from):  \n",
    "    \n",
    "    sentences = [i for i in tqdm(sentences) if len(i) > 2]\n",
    "    modelFT = FastText(sentences=sentences, size=100, min_count=1, window=5)\n",
    "    modelFT.save(\"ft_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузить модель\n",
    "\n",
    "modelFT = FastText.load(\"ft_model\")\n",
    "ft_index = annoy.AnnoyIndex(100 ,'angular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Размерность вектора модели. Если установить 100 — каждое слово в корпусе будет представлено в виде 100-мерного вектора, и т.п.\n",
    "#Наименьшее допустимое количество символов в слове, для которого будет создаваться векторное представление; так можно убрать частотные, но не очень значимые слова типа союзов и предлогов.\n",
    "#Размер окна. Этот параметр задает, сколько соседних слов считается частью контекста. Если выставить 40, то алгоритм возьмет 40 слов спереди от слова и 40 слов сзади от слова;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ваша',\n",
       " 'девятнадцать',\n",
       " 'действительно',\n",
       " 'сама',\n",
       " 'семнадцатый',\n",
       " 'нельзя',\n",
       " 'которая',\n",
       " 'этом',\n",
       " 'это',\n",
       " 'лучше',\n",
       " 'пора',\n",
       " 'около',\n",
       " 'тот',\n",
       " 'лет',\n",
       " 'тому',\n",
       " 'даром',\n",
       " 'шесть',\n",
       " 'том',\n",
       " 'всё',\n",
       " 'многочисленные']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(get_stop_words(\"ru\")))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем Индексы для вопросов-ответов\n",
    "\n",
    "file_path_from = 'speaker.ann'\n",
    "if not os.path.isfile(file_path_from):  \n",
    "    morpher = MorphAnalyzer()\n",
    "    sw = set(get_stop_words(\"ru\"))\n",
    "    exclude = set(string.punctuation)\n",
    "    modelFT = FastText.load(\"ft_model\")\n",
    "    ft_index = annoy.AnnoyIndex(100 ,'angular')\n",
    "\n",
    "    index_map = {}\n",
    "    counter = 0\n",
    "    with open(\"Otvety2.txt\", \"r\") as f:\n",
    "        for line in tqdm(f):\n",
    "            n_ft = 0\n",
    "            spls = line.split(\"\\t\")\n",
    "            index_map[counter] = spls[1]\n",
    "            question = preprocess_txt(spls[0])\n",
    "            vector_ft = np.zeros(100)\n",
    "            for word in question:\n",
    "                if word in modelFT.wv:\n",
    "                    vector_ft += modelFT.wv[word]\n",
    "                    n_ft += 1\n",
    "            if n_ft > 0:\n",
    "                vector_ft = vector_ft / n_ft\n",
    "            ft_index.add_item(counter, vector_ft)\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            if counter > 50_000:\n",
    "                break\n",
    "\n",
    "    ft_index.build(10)\n",
    "    ft_index.save('speaker.ann')\n",
    "    \n",
    "    with open(\"index_speaker.pkl\", \"wb\") as f:\n",
    "        pickle.dump(index_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ofHcj4W8VJ0E",
    "outputId": "f40f185c-ae76-4c53-c08f-5428d1745c7b"
   },
   "outputs": [],
   "source": [
    "#  Загрузим индексы\n",
    "ft_index = annoy.AnnoyIndex(100, 'angular')\n",
    "ft_index.load('speaker.ann')\n",
    "index_map = pd.read_pickle(\"index_speaker.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69, 35, 71, 17, 78, 38, 46,  8, 88, 90, 85, 30, 29, 18, 32, 27, 25,\n",
       "       87, 62,  6, 44, 39, 56, 12, 53, 52, 10, 77, 95, 22, 94, 83, 67, 72,\n",
       "       48, 33, 55, 74, 36, 79, 68, 63, 45,  7, 51, 75, 50, 66,  4, 23, 64,\n",
       "       24, 41, 92, 76, 81, 14, 59, 97, 40, 60, 84, 86, 93, 11, 98, 80, 34,\n",
       "       58, 96, 13,  1, 19, 42, 99,  3, 54, 91,  0, 26, 70, 82, 21, 20, 57,\n",
       "       16, 37, 89, 43, 31, 65, 73, 49, 28,  5, 47, 15,  9, 61,  2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.permutation(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "b9DvlLGVVJ0F",
    "outputId": "27a213d5-372d-428b-de89-0b1f19af8a73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5468, 14291, 29194, 32866, 31658],\n",
       " [1.2329422235488892,\n",
       "  1.2337841987609863,\n",
       "  1.2337841987609863,\n",
       "  1.2337841987609863,\n",
       "  1.234498381614685])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Пример получения индексов\n",
    "a = ft_index.get_nns_by_vector(np.random.permutation(100), 5, include_distances=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на выходных закончились =). \\n',\n",
       " 'меню зависло. \\n',\n",
       " '2. Бельгийский аппарат для измерения силы детонации порохового заряда, вариант порохового тестера XVI века.<br>Самый ранний аппарат для проверки силы ружейного пороха был изобретен Берном в 1578 году. Это был небольшой цилиндр с прочно прилегающей крышкой на петле. Угол, на который поднималась крышка при взрыве внутри механизма, указывал силу пороха.<br><br>Непонятно. Зачем стоит скобка на нижней дуге механизма? Ограничитель засыпки пороха,<br>который снимается, затем, при тестировании?. \\n',\n",
       " 'угар. \\n',\n",
       " 'В любом продовольственном магазине Они могут быть под разными названиями<br>. \\n']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index_map[x] for x in a[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gbcdn.mrgcdn.ru/uploads/asset/5209459/attachment/1b2f5aa57ff77e7c2d2ee26ceb09eb9e.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Z-qxfw70VJ0G",
    "outputId": "d9ad78f5-5ed3-4690-e5d4-2d05ba34155c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61ed298d9354658aa82f84c4582db97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>descrirption</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>subcategory_id</th>\n",
       "      <th>properties</th>\n",
       "      <th>image_links</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Юбка детская ORBY</td>\n",
       "      <td>Новая, не носили ни разу. В реале красивей чем...</td>\n",
       "      <td>58e3cfe6132ca50e053f5f82</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2211</td>\n",
       "      <td>{'detskie_razmer_rost': '81-86 (1,5 года)'}</td>\n",
       "      <td>http://cache3.youla.io/files/images/360_360/58...</td>\n",
       "      <td>[юбка, детский, orby, новый, носить, реал, кра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ботильоны</td>\n",
       "      <td>Новые,привезены из Чехии ,указан размер 40,но ...</td>\n",
       "      <td>5667531b2b7f8d127d838c34</td>\n",
       "      <td>9.0</td>\n",
       "      <td>902</td>\n",
       "      <td>{'zhenskaya_odezhda_tzvet': 'Зеленый', 'visota...</td>\n",
       "      <td>http://cache3.youla.io/files/images/360_360/5b...</td>\n",
       "      <td>[ботильон, новыепривезти, чехия, указать, разм...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Брюки</td>\n",
       "      <td>Размер 40-42. Брюки почти новые - не знаю как ...</td>\n",
       "      <td>59534826aaab284cba337e06</td>\n",
       "      <td>9.0</td>\n",
       "      <td>906</td>\n",
       "      <td>{'zhenskaya_odezhda_dzhinsy_bryuki_tip': 'Брюк...</td>\n",
       "      <td>http://cache3.youla.io/files/images/360_360/59...</td>\n",
       "      <td>[брюки, размер, 4042, брюки, новый, знать, мер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Продам детские шапки</td>\n",
       "      <td>Продам шапки,кажда 200р.Розовая и белая проданны.</td>\n",
       "      <td>57de544096ad842e26de8027</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2217</td>\n",
       "      <td>{'detskie_pol': 'Девочкам', 'detskaya_odezhda_...</td>\n",
       "      <td>http://cache3.youla.io/files/images/360_360/57...</td>\n",
       "      <td>[продать, детский, шапка, продать, шапкикажда,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Блузка</td>\n",
       "      <td>Темно-синяя, 42 размер,состояние отличное,как ...</td>\n",
       "      <td>5ad4d2626c86cb168d212022</td>\n",
       "      <td>9.0</td>\n",
       "      <td>907</td>\n",
       "      <td>{'zhenskaya_odezhda_tzvet': 'Синий', 'zhenskay...</td>\n",
       "      <td>http://cache3.youla.io/files/images/360_360/5a...</td>\n",
       "      <td>[блузка, темносиний, 42, размерсостояние, отли...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title                                       descrirption  \\\n",
       "0     Юбка детская ORBY  Новая, не носили ни разу. В реале красивей чем...   \n",
       "1             Ботильоны  Новые,привезены из Чехии ,указан размер 40,но ...   \n",
       "2                 Брюки  Размер 40-42. Брюки почти новые - не знаю как ...   \n",
       "3  Продам детские шапки  Продам шапки,кажда 200р.Розовая и белая проданны.   \n",
       "4                Блузка  Темно-синяя, 42 размер,состояние отличное,как ...   \n",
       "\n",
       "                 product_id  category_id subcategory_id  \\\n",
       "0  58e3cfe6132ca50e053f5f82         22.0           2211   \n",
       "1  5667531b2b7f8d127d838c34          9.0            902   \n",
       "2  59534826aaab284cba337e06          9.0            906   \n",
       "3  57de544096ad842e26de8027         22.0           2217   \n",
       "4  5ad4d2626c86cb168d212022          9.0            907   \n",
       "\n",
       "                                          properties  \\\n",
       "0        {'detskie_razmer_rost': '81-86 (1,5 года)'}   \n",
       "1  {'zhenskaya_odezhda_tzvet': 'Зеленый', 'visota...   \n",
       "2  {'zhenskaya_odezhda_dzhinsy_bryuki_tip': 'Брюк...   \n",
       "3  {'detskie_pol': 'Девочкам', 'detskaya_odezhda_...   \n",
       "4  {'zhenskaya_odezhda_tzvet': 'Синий', 'zhenskay...   \n",
       "\n",
       "                                         image_links  \\\n",
       "0  http://cache3.youla.io/files/images/360_360/58...   \n",
       "1  http://cache3.youla.io/files/images/360_360/5b...   \n",
       "2  http://cache3.youla.io/files/images/360_360/59...   \n",
       "3  http://cache3.youla.io/files/images/360_360/57...   \n",
       "4  http://cache3.youla.io/files/images/360_360/5a...   \n",
       "\n",
       "                                                text  \n",
       "0  [юбка, детский, orby, новый, носить, реал, кра...  \n",
       "1  [ботильон, новыепривезти, чехия, указать, разм...  \n",
       "2  [брюки, размер, 4042, брюки, новый, знать, мер...  \n",
       "3  [продать, детский, шапка, продать, шапкикажда,...  \n",
       "4  [блузка, темносиний, 42, размерсостояние, отли...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создадим модель продуктовых данных\n",
    "\n",
    "shop_data = pd.read_csv(\"ProductsDataset.csv\")\n",
    "#shop_data = shop_data.iloc[:5000, :]\n",
    "\n",
    "shop_data['text'] = shop_data['title'] + \" \" + shop_data[\"descrirption\"]\n",
    "shop_data['text'] = shop_data['text'].progress_apply(lambda x: preprocess_txt(str(x)))\n",
    "shop_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rqml_x5JVJ0H"
   },
   "outputs": [],
   "source": [
    "# Подготовка для создания модели для определения домена данных\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Xfxh1xO9VJ0I"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87eb914c9ad24f9d83755a3859f7d2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a31a2c197e481cbb0f3f7a41089cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idxs = set(np.random.randint(0, len(index_map), len(shop_data)))\n",
    "# Вопрос-ответный домен\n",
    "negative_texts = [\" \".join(preprocess_txt(index_map[i])) for i in tqdm(idxs)]\n",
    "# Продуктовый домен\n",
    "positive_texts = [\" \".join(val) for val in tqdm(shop_data['text'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "3ud68payVJ0J"
   },
   "outputs": [],
   "source": [
    "# ВО = 0, Прод = 1\n",
    "\n",
    "dataset = negative_texts + positive_texts\n",
    "labels = np.zeros(len(dataset))\n",
    "labels[len(negative_texts):] = np.ones(len(positive_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "N_oxnOuzVJ0K"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, \n",
    "                                                    stratify=labels, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "tNzqfsIZVJ0K",
    "outputId": "d3173275-3a00-47e2-8243-372f61a627c5"
   },
   "outputs": [],
   "source": [
    "# Модель\n",
    "\n",
    "x_train_vec = vectorizer.fit_transform(X_train)\n",
    "x_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression().fit(x_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "q_bWwksAVJ0L",
    "outputId": "e410d691-e516-45aa-ac0e-d8ef0378cc4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9760302085043506"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Качество\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true=y_test, y_pred=lr.predict(x_test_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23VbtYIiVJ0M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим IDF взвешивание (для каждого слова найдем IDF вес)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "XFgbubh7VJ0N"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "yx8AkUiiVJ0N",
    "outputId": "dce321cc-2846-4c1c-a42e-c6aff189837e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.771440512796"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tfidf_vect.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "aQQG_bluVJ0O",
    "outputId": "90701b9a-a491-417a-a291-fc3209df90fb"
   },
   "outputs": [],
   "source": [
    "idfs = {v[0]: v[1] for v in zip(tfidf_vect.vocabulary_, tfidf_vect.idf_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150383"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['новый', 'куртка', 'adidas', 'демисезонный', 'фирма']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(idfs.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.309122964599505,\n",
       " 6.981845259015088,\n",
       " 10.184591701953405,\n",
       " 11.100882433827561,\n",
       " 11.100882433827561]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(idfs.values())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "9ae8735faac34a36be98a6d1f1fcd62e"
     ]
    },
    "id": "JjWyFjJBVJ0P",
    "outputId": "69c1e269-b020-4542-9b84-812cc5485b08"
   },
   "outputs": [],
   "source": [
    "# Создаем Индексы для продуктовых данных\n",
    "\n",
    "file_path_from = 'shop.ann'\n",
    "if not os.path.isfile(file_path_from):  \n",
    "    \n",
    "    \n",
    "    ft_index_shop = annoy.AnnoyIndex(100 ,'angular')\n",
    "\n",
    "    midf = np.mean(tfidf_vect.idf_)\n",
    "\n",
    "    index_map_shop = {}\n",
    "    counter = 0\n",
    "\n",
    "    for i in tqdm(range(len(shop_data))):\n",
    "        n_ft = 0\n",
    "        index_map_shop[counter] = (shop_data.loc[i, \"title\"], shop_data.loc[i, \"image_links\"])\n",
    "        vector_ft = np.zeros(100)\n",
    "        for word in shop_data.loc[i, \"text\"]:\n",
    "            if word in modelFT.wv:\n",
    "                vector_ft += modelFT.wv[word] * idfs.get(word, midf)\n",
    "                n_ft += idfs.get(word, midf)\n",
    "        if n_ft > 0:\n",
    "            vector_ft = vector_ft / n_ft\n",
    "        ft_index_shop.add_item(counter, vector_ft)\n",
    "        counter += 1\n",
    "\n",
    "    ft_index_shop.build(10)\n",
    "    ft_index_shop.save('shop.ann')\n",
    "\n",
    "    file_path_from = 'index_shop.pkl'\n",
    "    if not os.path.isfile(file_path_from):  \n",
    "    \n",
    "        with open(\"index_shop.pkl\", \"wb\") as f:\n",
    "            pickle.dump(index_map_shop, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим индексы\n",
    "\n",
    "midf = np.mean(tfidf_vect.idf_)\n",
    "\n",
    "ft_index_shop = annoy.AnnoyIndex(100, 'angular')\n",
    "ft_index_shop.load('shop.ann') \n",
    "\n",
    "index_map_shop = pd.read_pickle(\"index_shop.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iwnz2qFTVJ0Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "47tviu39VJ0Q"
   },
   "outputs": [],
   "source": [
    "# Основная функция преобразования текста в вектор х100\n",
    "\n",
    "def embed_txt(txt, idfs, midf):\n",
    "    n_ft = 0\n",
    "    vector_ft = np.zeros(100)\n",
    "    for word in txt:\n",
    "        if word in modelFT.wv:\n",
    "            vector_ft += modelFT.wv[word] * idfs.get(word, midf)\n",
    "            n_ft += idfs.get(word, midf)\n",
    "    return vector_ft / n_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "H3jf5wMsVJ0R",
    "outputId": "d6f9717b-3d10-472e-bc27-893bf19f21bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4460, 17938, 4897, 6088, 468],\n",
       " [1.20292329788208,\n",
       "  1.2040966749191284,\n",
       "  1.2047914266586304,\n",
       "  1.205664038658142,\n",
       "  1.2065114974975586])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример получения индекса\n",
    "\n",
    "ft_index_shop.get_nns_by_vector(np.ones(100)*20, 5, include_distances=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание своего бота в телеграмм\n",
    "# @botfather\n",
    "# /start\n",
    "# /newbot - create a new bot\n",
    "# name1\n",
    "# name1_BOT\n",
    "#. ->. API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "B3sxuG-hVJ0S",
    "outputId": "e61e1613-8860-4e1a-814a-eda3f75bd6ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4s/fd42fwqn6g746zjn0s08wmxm0000gn/T/ipykernel_1423/2641577093.py:10: RuntimeWarning: invalid value encountered in divide\n",
      "  return vector_ft / n_ft\n",
      "No error handlers are registered, logging exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lmv/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/telegram/ext/dispatcher.py\", line 442, in process_update\n",
      "    handler.handle_update(update, self, check, context)\n",
      "  File \"/Users/lmv/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/telegram/ext/handler.py\", line 160, in handle_update\n",
      "    return self.callback(update, context)\n",
      "  File \"/var/folders/4s/fd42fwqn6g746zjn0s08wmxm0000gn/T/ipykernel_1423/1419090691.py\", line 33, in textMessage\n",
      "    context.bot.send_message(chat_id=update.message.chat_id, text=index_map[ft_index_val[0]])\n",
      "  File \"/Users/lmv/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/telegram/bot.py\", line 126, in decorator\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/Users/lmv/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/telegram/bot.py\", line 474, in send_message\n",
      "    return self._message(  # type: ignore[return-value]\n",
      "  File \"/Users/lmv/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/telegram/bot.py\", line 295, in _message\n",
      "    result = self._post(endpoint, data, timeout=timeout, api_kwargs=api_kwargs)\n",
      "  File \"/Users/lmv/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/telegram/bot.py\", line 258, in _post\n",
      "    return self.request.post(\n",
      "  File \"/Users/lmv/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/telegram/utils/request.py\", line 349, in post\n",
      "    result = self._request_wrapper(\n",
      "  File \"/Users/lmv/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/telegram/utils/request.py\", line 272, in _request_wrapper\n",
      "    raise BadRequest(message)\n",
      "telegram.error.BadRequest: Message is too long\n"
     ]
    }
   ],
   "source": [
    "# заменить на свой токен\n",
    "updater = Updater(\"6523778836:AAH4__N3uT8Jk5DEX8yBpnD8rs0BAAInzsA\", use_context=True) # Токен API к Telegram\n",
    "dispatcher = updater.dispatcher\n",
    "\n",
    "def startCommand(update, context):\n",
    "    context.bot.send_message(chat_id=update.message.chat_id, text='Привет, давай пообщаемся?')\n",
    "\n",
    "def textMessage(update, context):\n",
    "    \n",
    "    input_txt = preprocess_txt(update.message.text)\n",
    "    vect = vectorizer.transform([\" \".join(input_txt)])\n",
    "    prediction = lr.predict(vect)\n",
    "    \n",
    "    # ПРОД\n",
    "    if prediction[0] == 1:\n",
    "        vect_ft = embed_txt(input_txt, idfs, midf)\n",
    "        ft_index_shop_val = ft_index_shop.get_nns_by_vector(vect_ft, 5)\n",
    "        for item in ft_index_shop_val:\n",
    "            title, image = index_map_shop[item]\n",
    "            context.bot.send_message(chat_id=update.message.chat_id, text=\"title: {} image: {}\".format(title, image))\n",
    "        return\n",
    "    \n",
    "    # QA\n",
    "    vect_ft = embed_txt(input_txt, {}, 1)\n",
    "    ft_index_val, distances = ft_index.get_nns_by_vector(vect_ft, 1, include_distances=True)\n",
    "    \n",
    "    # \n",
    "    if distances[0] > 100.5:\n",
    "        print(distances[0])\n",
    "        context.bot.send_message(chat_id=update.message.chat_id, text=\"Моя твоя не понимать\")\n",
    "        return\n",
    "    \n",
    "    # Вопрос-Ответ\n",
    "    context.bot.send_message(chat_id=update.message.chat_id, text=index_map[ft_index_val[0]])\n",
    "        \n",
    "start_command_handler = CommandHandler('start', startCommand)\n",
    "text_message_handler = MessageHandler(Filters.text, textMessage)\n",
    "dispatcher.add_handler(start_command_handler)\n",
    "dispatcher.add_handler(text_message_handler)\n",
    "updater.start_polling(clean=True)\n",
    "updater.idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://github.com/Koziev/NLP_Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "lesson_16.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
